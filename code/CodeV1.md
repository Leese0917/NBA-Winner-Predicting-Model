# <b>Midterm Report</b>

## Introduction/Background:
----------------------------------
With basketball being a popular pastime in the United States, fans spend much time on  supplementary activities such as betting. Sports betting allows fans to engage at a deeper level - leading them to wager money and track stats, storylines, and performances at a game-by-game level. 

The team plans to explore applications of machine learning to sports betting as it relates to the NBA season. Similar efforts in the past have successfully combined machine learning concepts such as convolutional neural networks with modern portfolio theory to maximize predictive power,  as well as expected profit on NBA betting (Hubáček, 2019). Other examples in sports such as tennis have seen the use of ensemble methods incorporating models such as logistic regression, neural networks, random forests, GBM, and SVM maximize expected return (Wilkens, 2021).

The dataset that we will use consists of data from each of the regular and postseason NBA games from the 2007-2008 to the 2021-2022 seasons (Skompinski, 2021). For each game, there are several attributes (100+) involving statistics for each of the teams playing, in addition to some betting data. 

## Problem Definition:
----------------------------------
One of the most common forms of sports betting is on the winner of a given matchup. Their simplicity and frequency drives popularity among bettors. Therefore, we considered analyzing winners of individual matches throughout the season.

While predicting regular season wins would be a useful tool, we find it more compelling to make forecasts on the winners of playoff matchups, based on regular season data. Teams are often criticized for being “regular season” performers - lacking an “intangible” gear to go into when the heightened postseason atmosphere materializes. More attention and prominence is associated with betting in the playoffs, and oftentimes greater payouts - further solidifying the exercise as a worthwhile one. 

## Methods:
----------------------------------
Using our aforementioned NBA game data, we plan on using multiple methods in order to build our predictive model. Namely, we will create a <font color = '419CFF'>convolutional neural network</font> and use a brute force strategy in order to analyze statistics from previous matches and generate a prediction. Additionally, we want to approach the <font color = '419CFF'>data with PCA (Preprocessing Analysis)</font> in order to find metrics for optimal predicted value for game winners. We may also experiment with player datasets in order to refine the model


```python
# imports
import numpy as np
import pandas as pd
from tensorflow.keras import datasets, layers, models, losses
import matplotlib.pyplot as plt
from sklearn import preprocessing, decomposition, metrics
from sklearn.metrics import accuracy_score, mean_squared_error, roc_curve, roc_auc_score
```


```python
# Load the data that NBA games from the 2007-2008 to 2021-2022 seasons
df = pd.read_csv('DatasetFinal.csv')
df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0.1</th>
      <th>Unnamed: 0</th>
      <th>Post-Season</th>
      <th>TEAM_NAME</th>
      <th>GP</th>
      <th>W</th>
      <th>L</th>
      <th>W_PCT</th>
      <th>MIN</th>
      <th>FGM</th>
      <th>FGA</th>
      <th>FG_PCT</th>
      <th>FG3M</th>
      <th>FG3A</th>
      <th>FG3_PCT</th>
      <th>FTM</th>
      <th>FTA</th>
      <th>FT_PCT</th>
      <th>OREB</th>
      <th>DREB</th>
      <th>REB</th>
      <th>AST</th>
      <th>TOV</th>
      <th>STL</th>
      <th>BLK</th>
      <th>BLKA</th>
      <th>PF</th>
      <th>PFD</th>
      <th>PTS</th>
      <th>PLUS_MINUS</th>
      <th>GP_RANK</th>
      <th>W_RANK</th>
      <th>L_RANK</th>
      <th>W_PCT_RANK</th>
      <th>MIN_RANK</th>
      <th>FGM_RANK</th>
      <th>FGA_RANK</th>
      <th>FG_PCT_RANK</th>
      <th>FG3M_RANK</th>
      <th>FG3A_RANK</th>
      <th>FG3_PCT_RANK</th>
      <th>FTM_RANK</th>
      <th>FTA_RANK</th>
      <th>FT_PCT_RANK</th>
      <th>OREB_RANK</th>
      <th>DREB_RANK</th>
      <th>REB_RANK</th>
      <th>AST_RANK</th>
      <th>TOV_RANK</th>
      <th>STL_RANK</th>
      <th>BLK_RANK</th>
      <th>BLKA_RANK</th>
      <th>PF_RANK</th>
      <th>PFD_RANK</th>
      <th>PTS_RANK</th>
      <th>PLUS_MINUS_RANK</th>
      <th>Date</th>
      <th>TEAM_NAME.1</th>
      <th>GP.1</th>
      <th>W.1</th>
      <th>L.1</th>
      <th>W_PCT.1</th>
      <th>MIN.1</th>
      <th>FGM.1</th>
      <th>FGA.1</th>
      <th>FG_PCT.1</th>
      <th>FG3M.1</th>
      <th>FG3A.1</th>
      <th>FG3_PCT.1</th>
      <th>FTM.1</th>
      <th>FTA.1</th>
      <th>FT_PCT.1</th>
      <th>OREB.1</th>
      <th>DREB.1</th>
      <th>REB.1</th>
      <th>AST.1</th>
      <th>TOV.1</th>
      <th>STL.1</th>
      <th>BLK.1</th>
      <th>BLKA.1</th>
      <th>PF.1</th>
      <th>PFD.1</th>
      <th>PTS.1</th>
      <th>PLUS_MINUS.1</th>
      <th>GP_RANK.1</th>
      <th>W_RANK.1</th>
      <th>L_RANK.1</th>
      <th>W_PCT_RANK.1</th>
      <th>MIN_RANK.1</th>
      <th>FGM_RANK.1</th>
      <th>FGA_RANK.1</th>
      <th>FG_PCT_RANK.1</th>
      <th>FG3M_RANK.1</th>
      <th>FG3A_RANK.1</th>
      <th>FG3_PCT_RANK.1</th>
      <th>FTM_RANK.1</th>
      <th>FTA_RANK.1</th>
      <th>FT_PCT_RANK.1</th>
      <th>OREB_RANK.1</th>
      <th>DREB_RANK.1</th>
      <th>REB_RANK.1</th>
      <th>AST_RANK.1</th>
      <th>TOV_RANK.1</th>
      <th>STL_RANK.1</th>
      <th>BLK_RANK.1</th>
      <th>BLKA_RANK.1</th>
      <th>PF_RANK.1</th>
      <th>PFD_RANK.1</th>
      <th>PTS_RANK.1</th>
      <th>PLUS_MINUS_RANK.1</th>
      <th>Date.1</th>
      <th>Score</th>
      <th>Home-Team-Win</th>
      <th>OU</th>
      <th>OU-Cover</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>Philadelphia 76ers</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>0.500</td>
      <td>48.0</td>
      <td>36.5</td>
      <td>82.5</td>
      <td>0.442</td>
      <td>4.0</td>
      <td>14.0</td>
      <td>0.286</td>
      <td>19.5</td>
      <td>28.5</td>
      <td>0.684</td>
      <td>16.5</td>
      <td>34.0</td>
      <td>50.5</td>
      <td>20.0</td>
      <td>19.0</td>
      <td>6.0</td>
      <td>8.0</td>
      <td>5.5</td>
      <td>19.5</td>
      <td>21.5</td>
      <td>96.5</td>
      <td>1.0</td>
      <td>2</td>
      <td>8</td>
      <td>12</td>
      <td>12</td>
      <td>5</td>
      <td>18</td>
      <td>14</td>
      <td>18</td>
      <td>26</td>
      <td>26</td>
      <td>26</td>
      <td>14</td>
      <td>13</td>
      <td>21</td>
      <td>3</td>
      <td>12</td>
      <td>5</td>
      <td>18</td>
      <td>26</td>
      <td>19</td>
      <td>4</td>
      <td>12</td>
      <td>7</td>
      <td>20</td>
      <td>20</td>
      <td>15</td>
      <td>2007-11-03</td>
      <td>New Jersey Nets</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>0.500</td>
      <td>50.5</td>
      <td>29.0</td>
      <td>75.0</td>
      <td>0.387</td>
      <td>7.5</td>
      <td>22.0</td>
      <td>0.341</td>
      <td>25.0</td>
      <td>29.5</td>
      <td>0.847</td>
      <td>11.0</td>
      <td>30.0</td>
      <td>41.0</td>
      <td>21.5</td>
      <td>18.0</td>
      <td>6.5</td>
      <td>6.5</td>
      <td>4.0</td>
      <td>26.5</td>
      <td>24.5</td>
      <td>90.5</td>
      <td>-14.0</td>
      <td>2</td>
      <td>8</td>
      <td>12</td>
      <td>12</td>
      <td>1</td>
      <td>30</td>
      <td>28</td>
      <td>29</td>
      <td>9</td>
      <td>7</td>
      <td>17</td>
      <td>4</td>
      <td>12</td>
      <td>3</td>
      <td>13</td>
      <td>20</td>
      <td>20</td>
      <td>10</td>
      <td>23</td>
      <td>17</td>
      <td>7</td>
      <td>7</td>
      <td>24</td>
      <td>9</td>
      <td>28</td>
      <td>27</td>
      <td>2007-11-03</td>
      <td>181</td>
      <td>0</td>
      <td>186.5</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>Washington Wizards</td>
      <td>2</td>
      <td>0</td>
      <td>2</td>
      <td>0.000</td>
      <td>50.5</td>
      <td>32.0</td>
      <td>89.5</td>
      <td>0.358</td>
      <td>3.0</td>
      <td>18.0</td>
      <td>0.167</td>
      <td>29.5</td>
      <td>40.0</td>
      <td>0.738</td>
      <td>18.5</td>
      <td>32.0</td>
      <td>50.5</td>
      <td>13.0</td>
      <td>19.0</td>
      <td>6.0</td>
      <td>5.5</td>
      <td>6.0</td>
      <td>26.5</td>
      <td>28.5</td>
      <td>96.5</td>
      <td>-14.5</td>
      <td>2</td>
      <td>20</td>
      <td>23</td>
      <td>20</td>
      <td>1</td>
      <td>28</td>
      <td>4</td>
      <td>30</td>
      <td>29</td>
      <td>16</td>
      <td>30</td>
      <td>2</td>
      <td>2</td>
      <td>14</td>
      <td>1</td>
      <td>16</td>
      <td>5</td>
      <td>30</td>
      <td>26</td>
      <td>19</td>
      <td>15</td>
      <td>13</td>
      <td>24</td>
      <td>2</td>
      <td>20</td>
      <td>28</td>
      <td>2007-11-03</td>
      <td>Orlando Magic</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>0.500</td>
      <td>48.0</td>
      <td>31.5</td>
      <td>75.0</td>
      <td>0.420</td>
      <td>10.0</td>
      <td>20.0</td>
      <td>0.500</td>
      <td>24.0</td>
      <td>34.0</td>
      <td>0.706</td>
      <td>9.5</td>
      <td>28.5</td>
      <td>38.0</td>
      <td>15.5</td>
      <td>11.0</td>
      <td>3.5</td>
      <td>5.5</td>
      <td>6.0</td>
      <td>21.0</td>
      <td>25.5</td>
      <td>97.0</td>
      <td>-2.5</td>
      <td>2</td>
      <td>8</td>
      <td>12</td>
      <td>12</td>
      <td>5</td>
      <td>29</td>
      <td>28</td>
      <td>21</td>
      <td>3</td>
      <td>13</td>
      <td>3</td>
      <td>7</td>
      <td>5</td>
      <td>19</td>
      <td>21</td>
      <td>23</td>
      <td>25</td>
      <td>28</td>
      <td>1</td>
      <td>29</td>
      <td>15</td>
      <td>13</td>
      <td>10</td>
      <td>6</td>
      <td>19</td>
      <td>16</td>
      <td>2007-11-03</td>
      <td>176</td>
      <td>0</td>
      <td>199.0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>2</td>
      <td>0</td>
      <td>Memphis Grizzlies</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0.000</td>
      <td>48.0</td>
      <td>38.0</td>
      <td>80.0</td>
      <td>0.475</td>
      <td>10.0</td>
      <td>23.0</td>
      <td>0.435</td>
      <td>15.0</td>
      <td>20.0</td>
      <td>0.750</td>
      <td>6.0</td>
      <td>36.0</td>
      <td>42.0</td>
      <td>21.0</td>
      <td>14.0</td>
      <td>5.0</td>
      <td>6.0</td>
      <td>5.0</td>
      <td>19.0</td>
      <td>17.0</td>
      <td>101.0</td>
      <td>-3.0</td>
      <td>24</td>
      <td>20</td>
      <td>12</td>
      <td>20</td>
      <td>5</td>
      <td>11</td>
      <td>20</td>
      <td>9</td>
      <td>3</td>
      <td>5</td>
      <td>10</td>
      <td>21</td>
      <td>23</td>
      <td>12</td>
      <td>29</td>
      <td>7</td>
      <td>18</td>
      <td>11</td>
      <td>8</td>
      <td>25</td>
      <td>10</td>
      <td>9</td>
      <td>5</td>
      <td>28</td>
      <td>15</td>
      <td>17</td>
      <td>2007-11-03</td>
      <td>Indiana Pacers</td>
      <td>2</td>
      <td>2</td>
      <td>0</td>
      <td>1.000</td>
      <td>50.5</td>
      <td>35.5</td>
      <td>87.5</td>
      <td>0.406</td>
      <td>10.0</td>
      <td>22.0</td>
      <td>0.455</td>
      <td>22.0</td>
      <td>28.5</td>
      <td>0.772</td>
      <td>13.5</td>
      <td>39.5</td>
      <td>53.0</td>
      <td>21.0</td>
      <td>15.5</td>
      <td>10.5</td>
      <td>9.5</td>
      <td>7.5</td>
      <td>25.5</td>
      <td>22.5</td>
      <td>103.0</td>
      <td>5.5</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>21</td>
      <td>6</td>
      <td>25</td>
      <td>3</td>
      <td>7</td>
      <td>8</td>
      <td>10</td>
      <td>13</td>
      <td>8</td>
      <td>11</td>
      <td>1</td>
      <td>1</td>
      <td>11</td>
      <td>15</td>
      <td>4</td>
      <td>1</td>
      <td>22</td>
      <td>22</td>
      <td>15</td>
      <td>10</td>
      <td>11</td>
      <td>2007-11-03</td>
      <td>232</td>
      <td>0</td>
      <td>211.5</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>3</td>
      <td>0</td>
      <td>Milwaukee Bucks</td>
      <td>2</td>
      <td>0</td>
      <td>2</td>
      <td>0.000</td>
      <td>48.0</td>
      <td>37.5</td>
      <td>87.0</td>
      <td>0.431</td>
      <td>4.5</td>
      <td>15.0</td>
      <td>0.300</td>
      <td>11.5</td>
      <td>18.0</td>
      <td>0.639</td>
      <td>16.0</td>
      <td>30.0</td>
      <td>46.0</td>
      <td>18.0</td>
      <td>17.0</td>
      <td>5.5</td>
      <td>6.0</td>
      <td>8.0</td>
      <td>30.0</td>
      <td>22.5</td>
      <td>91.0</td>
      <td>-11.0</td>
      <td>2</td>
      <td>20</td>
      <td>23</td>
      <td>20</td>
      <td>5</td>
      <td>14</td>
      <td>7</td>
      <td>20</td>
      <td>25</td>
      <td>21</td>
      <td>24</td>
      <td>28</td>
      <td>27</td>
      <td>27</td>
      <td>4</td>
      <td>20</td>
      <td>10</td>
      <td>26</td>
      <td>20</td>
      <td>23</td>
      <td>10</td>
      <td>24</td>
      <td>30</td>
      <td>15</td>
      <td>26</td>
      <td>24</td>
      <td>2007-11-03</td>
      <td>Chicago Bulls</td>
      <td>2</td>
      <td>0</td>
      <td>2</td>
      <td>0.000</td>
      <td>50.5</td>
      <td>35.5</td>
      <td>91.0</td>
      <td>0.390</td>
      <td>7.5</td>
      <td>23.5</td>
      <td>0.319</td>
      <td>15.5</td>
      <td>22.5</td>
      <td>0.689</td>
      <td>15.5</td>
      <td>25.0</td>
      <td>40.5</td>
      <td>20.5</td>
      <td>16.0</td>
      <td>9.0</td>
      <td>5.0</td>
      <td>7.0</td>
      <td>26.5</td>
      <td>26.5</td>
      <td>94.0</td>
      <td>-10.0</td>
      <td>2</td>
      <td>20</td>
      <td>23</td>
      <td>20</td>
      <td>1</td>
      <td>21</td>
      <td>2</td>
      <td>28</td>
      <td>9</td>
      <td>4</td>
      <td>21</td>
      <td>20</td>
      <td>19</td>
      <td>20</td>
      <td>5</td>
      <td>29</td>
      <td>22</td>
      <td>16</td>
      <td>16</td>
      <td>9</td>
      <td>19</td>
      <td>20</td>
      <td>24</td>
      <td>4</td>
      <td>23</td>
      <td>23</td>
      <td>2007-11-03</td>
      <td>150</td>
      <td>1</td>
      <td>193.5</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>4</td>
      <td>0</td>
      <td>Houston Rockets</td>
      <td>2</td>
      <td>2</td>
      <td>0</td>
      <td>1.000</td>
      <td>48.0</td>
      <td>36.5</td>
      <td>77.0</td>
      <td>0.474</td>
      <td>5.5</td>
      <td>19.0</td>
      <td>0.289</td>
      <td>22.0</td>
      <td>30.5</td>
      <td>0.721</td>
      <td>10.5</td>
      <td>34.5</td>
      <td>45.0</td>
      <td>21.0</td>
      <td>16.5</td>
      <td>9.5</td>
      <td>6.5</td>
      <td>3.5</td>
      <td>27.0</td>
      <td>24.5</td>
      <td>100.5</td>
      <td>6.5</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>5</td>
      <td>18</td>
      <td>25</td>
      <td>10</td>
      <td>23</td>
      <td>14</td>
      <td>25</td>
      <td>10</td>
      <td>10</td>
      <td>17</td>
      <td>17</td>
      <td>9</td>
      <td>13</td>
      <td>11</td>
      <td>18</td>
      <td>7</td>
      <td>7</td>
      <td>5</td>
      <td>28</td>
      <td>9</td>
      <td>18</td>
      <td>9</td>
      <td>2007-11-03</td>
      <td>Portland Trail Blazers</td>
      <td>2</td>
      <td>0</td>
      <td>2</td>
      <td>0.000</td>
      <td>48.0</td>
      <td>39.5</td>
      <td>80.5</td>
      <td>0.491</td>
      <td>6.0</td>
      <td>13.0</td>
      <td>0.462</td>
      <td>10.0</td>
      <td>13.5</td>
      <td>0.741</td>
      <td>8.5</td>
      <td>26.5</td>
      <td>35.0</td>
      <td>19.0</td>
      <td>15.0</td>
      <td>2.5</td>
      <td>2.5</td>
      <td>3.5</td>
      <td>18.5</td>
      <td>19.5</td>
      <td>95.0</td>
      <td>-14.5</td>
      <td>2</td>
      <td>20</td>
      <td>23</td>
      <td>20</td>
      <td>5</td>
      <td>7</td>
      <td>18</td>
      <td>4</td>
      <td>20</td>
      <td>27</td>
      <td>6</td>
      <td>29</td>
      <td>30</td>
      <td>13</td>
      <td>25</td>
      <td>28</td>
      <td>29</td>
      <td>23</td>
      <td>10</td>
      <td>30</td>
      <td>29</td>
      <td>5</td>
      <td>2</td>
      <td>22</td>
      <td>22</td>
      <td>28</td>
      <td>2007-11-03</td>
      <td>169</td>
      <td>1</td>
      <td>203.5</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>18838</th>
      <td>18838</td>
      <td>18838</td>
      <td>1</td>
      <td>Golden State Warriors</td>
      <td>82</td>
      <td>53</td>
      <td>29</td>
      <td>0.646</td>
      <td>48.1</td>
      <td>40.5</td>
      <td>86.4</td>
      <td>0.469</td>
      <td>14.3</td>
      <td>39.4</td>
      <td>0.364</td>
      <td>15.6</td>
      <td>20.3</td>
      <td>0.769</td>
      <td>9.8</td>
      <td>35.7</td>
      <td>45.5</td>
      <td>27.1</td>
      <td>14.9</td>
      <td>8.8</td>
      <td>4.5</td>
      <td>3.9</td>
      <td>21.0</td>
      <td>18.0</td>
      <td>111.0</td>
      <td>5.5</td>
      <td>1</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>24</td>
      <td>17</td>
      <td>21</td>
      <td>10</td>
      <td>3</td>
      <td>3</td>
      <td>8</td>
      <td>26</td>
      <td>26</td>
      <td>17</td>
      <td>20</td>
      <td>2</td>
      <td>7</td>
      <td>5</td>
      <td>29</td>
      <td>4</td>
      <td>18</td>
      <td>2</td>
      <td>27</td>
      <td>29</td>
      <td>15</td>
      <td>5</td>
      <td>2022-06-06</td>
      <td>Boston Celtics</td>
      <td>82</td>
      <td>51</td>
      <td>31</td>
      <td>0.622</td>
      <td>48.5</td>
      <td>40.7</td>
      <td>87.4</td>
      <td>0.466</td>
      <td>13.2</td>
      <td>37.1</td>
      <td>0.356</td>
      <td>17.0</td>
      <td>20.9</td>
      <td>0.816</td>
      <td>10.5</td>
      <td>35.5</td>
      <td>46.1</td>
      <td>24.8</td>
      <td>13.6</td>
      <td>7.2</td>
      <td>5.8</td>
      <td>4.6</td>
      <td>18.5</td>
      <td>19.4</td>
      <td>111.8</td>
      <td>7.3</td>
      <td>1</td>
      <td>6</td>
      <td>6</td>
      <td>6</td>
      <td>2</td>
      <td>13</td>
      <td>18</td>
      <td>15</td>
      <td>8</td>
      <td>9</td>
      <td>14</td>
      <td>14</td>
      <td>24</td>
      <td>2</td>
      <td>11</td>
      <td>4</td>
      <td>5</td>
      <td>14</td>
      <td>13</td>
      <td>19</td>
      <td>2</td>
      <td>11</td>
      <td>5</td>
      <td>20</td>
      <td>12</td>
      <td>2</td>
      <td>2022-06-06</td>
      <td>195</td>
      <td>1</td>
      <td>212.5</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18839</th>
      <td>18839</td>
      <td>18839</td>
      <td>1</td>
      <td>Boston Celtics</td>
      <td>82</td>
      <td>51</td>
      <td>31</td>
      <td>0.622</td>
      <td>48.5</td>
      <td>40.7</td>
      <td>87.4</td>
      <td>0.466</td>
      <td>13.2</td>
      <td>37.1</td>
      <td>0.356</td>
      <td>17.0</td>
      <td>20.9</td>
      <td>0.816</td>
      <td>10.5</td>
      <td>35.5</td>
      <td>46.1</td>
      <td>24.8</td>
      <td>13.6</td>
      <td>7.2</td>
      <td>5.8</td>
      <td>4.6</td>
      <td>18.5</td>
      <td>19.4</td>
      <td>111.8</td>
      <td>7.3</td>
      <td>1</td>
      <td>6</td>
      <td>6</td>
      <td>6</td>
      <td>2</td>
      <td>13</td>
      <td>18</td>
      <td>15</td>
      <td>8</td>
      <td>9</td>
      <td>14</td>
      <td>14</td>
      <td>24</td>
      <td>2</td>
      <td>11</td>
      <td>4</td>
      <td>5</td>
      <td>14</td>
      <td>13</td>
      <td>19</td>
      <td>2</td>
      <td>11</td>
      <td>5</td>
      <td>20</td>
      <td>12</td>
      <td>2</td>
      <td>2022-06-08</td>
      <td>Golden State Warriors</td>
      <td>82</td>
      <td>53</td>
      <td>29</td>
      <td>0.646</td>
      <td>48.1</td>
      <td>40.5</td>
      <td>86.4</td>
      <td>0.469</td>
      <td>14.3</td>
      <td>39.4</td>
      <td>0.364</td>
      <td>15.6</td>
      <td>20.3</td>
      <td>0.769</td>
      <td>9.8</td>
      <td>35.7</td>
      <td>45.5</td>
      <td>27.1</td>
      <td>14.9</td>
      <td>8.8</td>
      <td>4.5</td>
      <td>3.9</td>
      <td>21.0</td>
      <td>18.0</td>
      <td>111.0</td>
      <td>5.5</td>
      <td>1</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>24</td>
      <td>17</td>
      <td>21</td>
      <td>10</td>
      <td>3</td>
      <td>3</td>
      <td>8</td>
      <td>26</td>
      <td>26</td>
      <td>17</td>
      <td>20</td>
      <td>2</td>
      <td>7</td>
      <td>5</td>
      <td>29</td>
      <td>4</td>
      <td>18</td>
      <td>2</td>
      <td>27</td>
      <td>29</td>
      <td>15</td>
      <td>5</td>
      <td>2022-06-08</td>
      <td>216</td>
      <td>1</td>
      <td>212.5</td>
      <td>1</td>
    </tr>
    <tr>
      <th>18840</th>
      <td>18840</td>
      <td>18840</td>
      <td>1</td>
      <td>Boston Celtics</td>
      <td>82</td>
      <td>51</td>
      <td>31</td>
      <td>0.622</td>
      <td>48.5</td>
      <td>40.7</td>
      <td>87.4</td>
      <td>0.466</td>
      <td>13.2</td>
      <td>37.1</td>
      <td>0.356</td>
      <td>17.0</td>
      <td>20.9</td>
      <td>0.816</td>
      <td>10.5</td>
      <td>35.5</td>
      <td>46.1</td>
      <td>24.8</td>
      <td>13.6</td>
      <td>7.2</td>
      <td>5.8</td>
      <td>4.6</td>
      <td>18.5</td>
      <td>19.4</td>
      <td>111.8</td>
      <td>7.3</td>
      <td>1</td>
      <td>6</td>
      <td>6</td>
      <td>6</td>
      <td>2</td>
      <td>13</td>
      <td>18</td>
      <td>15</td>
      <td>8</td>
      <td>9</td>
      <td>14</td>
      <td>14</td>
      <td>24</td>
      <td>2</td>
      <td>11</td>
      <td>4</td>
      <td>5</td>
      <td>14</td>
      <td>13</td>
      <td>19</td>
      <td>2</td>
      <td>11</td>
      <td>5</td>
      <td>20</td>
      <td>12</td>
      <td>2</td>
      <td>2022-06-10</td>
      <td>Golden State Warriors</td>
      <td>82</td>
      <td>53</td>
      <td>29</td>
      <td>0.646</td>
      <td>48.1</td>
      <td>40.5</td>
      <td>86.4</td>
      <td>0.469</td>
      <td>14.3</td>
      <td>39.4</td>
      <td>0.364</td>
      <td>15.6</td>
      <td>20.3</td>
      <td>0.769</td>
      <td>9.8</td>
      <td>35.7</td>
      <td>45.5</td>
      <td>27.1</td>
      <td>14.9</td>
      <td>8.8</td>
      <td>4.5</td>
      <td>3.9</td>
      <td>21.0</td>
      <td>18.0</td>
      <td>111.0</td>
      <td>5.5</td>
      <td>1</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>24</td>
      <td>17</td>
      <td>21</td>
      <td>10</td>
      <td>3</td>
      <td>3</td>
      <td>8</td>
      <td>26</td>
      <td>26</td>
      <td>17</td>
      <td>20</td>
      <td>2</td>
      <td>7</td>
      <td>5</td>
      <td>29</td>
      <td>4</td>
      <td>18</td>
      <td>2</td>
      <td>27</td>
      <td>29</td>
      <td>15</td>
      <td>5</td>
      <td>2022-06-10</td>
      <td>204</td>
      <td>0</td>
      <td>212.5</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18841</th>
      <td>18841</td>
      <td>18841</td>
      <td>1</td>
      <td>Golden State Warriors</td>
      <td>82</td>
      <td>53</td>
      <td>29</td>
      <td>0.646</td>
      <td>48.1</td>
      <td>40.5</td>
      <td>86.4</td>
      <td>0.469</td>
      <td>14.3</td>
      <td>39.4</td>
      <td>0.364</td>
      <td>15.6</td>
      <td>20.3</td>
      <td>0.769</td>
      <td>9.8</td>
      <td>35.7</td>
      <td>45.5</td>
      <td>27.1</td>
      <td>14.9</td>
      <td>8.8</td>
      <td>4.5</td>
      <td>3.9</td>
      <td>21.0</td>
      <td>18.0</td>
      <td>111.0</td>
      <td>5.5</td>
      <td>1</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>24</td>
      <td>17</td>
      <td>21</td>
      <td>10</td>
      <td>3</td>
      <td>3</td>
      <td>8</td>
      <td>26</td>
      <td>26</td>
      <td>17</td>
      <td>20</td>
      <td>2</td>
      <td>7</td>
      <td>5</td>
      <td>29</td>
      <td>4</td>
      <td>18</td>
      <td>2</td>
      <td>27</td>
      <td>29</td>
      <td>15</td>
      <td>5</td>
      <td>2022-06-13</td>
      <td>Boston Celtics</td>
      <td>82</td>
      <td>51</td>
      <td>31</td>
      <td>0.622</td>
      <td>48.5</td>
      <td>40.7</td>
      <td>87.4</td>
      <td>0.466</td>
      <td>13.2</td>
      <td>37.1</td>
      <td>0.356</td>
      <td>17.0</td>
      <td>20.9</td>
      <td>0.816</td>
      <td>10.5</td>
      <td>35.5</td>
      <td>46.1</td>
      <td>24.8</td>
      <td>13.6</td>
      <td>7.2</td>
      <td>5.8</td>
      <td>4.6</td>
      <td>18.5</td>
      <td>19.4</td>
      <td>111.8</td>
      <td>7.3</td>
      <td>1</td>
      <td>6</td>
      <td>6</td>
      <td>6</td>
      <td>2</td>
      <td>13</td>
      <td>18</td>
      <td>15</td>
      <td>8</td>
      <td>9</td>
      <td>14</td>
      <td>14</td>
      <td>24</td>
      <td>2</td>
      <td>11</td>
      <td>4</td>
      <td>5</td>
      <td>14</td>
      <td>13</td>
      <td>19</td>
      <td>2</td>
      <td>11</td>
      <td>5</td>
      <td>20</td>
      <td>12</td>
      <td>2</td>
      <td>2022-06-13</td>
      <td>198</td>
      <td>1</td>
      <td>212.5</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18842</th>
      <td>18842</td>
      <td>18842</td>
      <td>1</td>
      <td>Boston Celtics</td>
      <td>82</td>
      <td>51</td>
      <td>31</td>
      <td>0.622</td>
      <td>48.5</td>
      <td>40.7</td>
      <td>87.4</td>
      <td>0.466</td>
      <td>13.2</td>
      <td>37.1</td>
      <td>0.356</td>
      <td>17.0</td>
      <td>20.9</td>
      <td>0.816</td>
      <td>10.5</td>
      <td>35.5</td>
      <td>46.1</td>
      <td>24.8</td>
      <td>13.6</td>
      <td>7.2</td>
      <td>5.8</td>
      <td>4.6</td>
      <td>18.5</td>
      <td>19.4</td>
      <td>111.8</td>
      <td>7.3</td>
      <td>1</td>
      <td>6</td>
      <td>6</td>
      <td>6</td>
      <td>2</td>
      <td>13</td>
      <td>18</td>
      <td>15</td>
      <td>8</td>
      <td>9</td>
      <td>14</td>
      <td>14</td>
      <td>24</td>
      <td>2</td>
      <td>11</td>
      <td>4</td>
      <td>5</td>
      <td>14</td>
      <td>13</td>
      <td>19</td>
      <td>2</td>
      <td>11</td>
      <td>5</td>
      <td>20</td>
      <td>12</td>
      <td>2</td>
      <td>2022-06-16</td>
      <td>Golden State Warriors</td>
      <td>82</td>
      <td>53</td>
      <td>29</td>
      <td>0.646</td>
      <td>48.1</td>
      <td>40.5</td>
      <td>86.4</td>
      <td>0.469</td>
      <td>14.3</td>
      <td>39.4</td>
      <td>0.364</td>
      <td>15.6</td>
      <td>20.3</td>
      <td>0.769</td>
      <td>9.8</td>
      <td>35.7</td>
      <td>45.5</td>
      <td>27.1</td>
      <td>14.9</td>
      <td>8.8</td>
      <td>4.5</td>
      <td>3.9</td>
      <td>21.0</td>
      <td>18.0</td>
      <td>111.0</td>
      <td>5.5</td>
      <td>1</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>24</td>
      <td>17</td>
      <td>21</td>
      <td>10</td>
      <td>3</td>
      <td>3</td>
      <td>8</td>
      <td>26</td>
      <td>26</td>
      <td>17</td>
      <td>20</td>
      <td>2</td>
      <td>7</td>
      <td>5</td>
      <td>29</td>
      <td>4</td>
      <td>18</td>
      <td>2</td>
      <td>27</td>
      <td>29</td>
      <td>15</td>
      <td>5</td>
      <td>2022-06-16</td>
      <td>193</td>
      <td>0</td>
      <td>212.5</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>18843 rows × 115 columns</p>
</div>



### Data cleaning

In order to clean our data, we made several modifications. As the table includes game data from the 2007-2008 to the 2021-2022 NBA season across both regular season and playoff matchups, there are nearly 20,000 rows including over 100 fields. Save for the last four columns, most fields appear in duplicate - this is because they are shown for both teams (e.g., GP - Games Played, FGM - Field Goals Made, REB - Rebounds, etc.). 

In order to distinguish between fields corresponding to the home team versus the away team, we concatenate a “.1” to the last 54 fields to designate the away team. While our dataset is relatively robust, we did see an opportunity to introduce some labels: namely, a binary “playoff” field.. However, the distinction between playoff and regular season games is not made clear in the data, so we introduced this column to the dataset. Additionally, we find it potentially valuable to add win/loss streak field(s) to demonstrate the effect of momentum on a team’s success. We have not yet implemented this attribute, but plan to for the final iteration.

We also removed the first column (index) from the dataset, as it is made redundant by the default indexing of the pandas dataframe we work with. Furthermore, we removed the Date column - the reasons for this are twofold. One, the date column intuitively should have little impact on the winner of a game, as the effect of progression through the season (e.g., fatigue, push for playoff seeding, tanking) should be captured by Games Played and Win/Loss and two, is formatted such that it would be difficult to work with and incorporate into our Principal Component Analysis. Before executing the PCA, however, we needed to encode two categorical variables (Team_Name and Team_Name.1), instead denoting each team with an integer value from 1-30. 


```python
df.columns
df.drop(columns=df.columns[0:2],inplace=True)

# exclude dates
df = df.drop(columns=['Date.1', 'Date'])

# encoding the team names
enc = preprocessing.LabelEncoder()
df[['TEAM_NAME', 'TEAM_NAME.1']] = df[['TEAM_NAME', 'TEAM_NAME.1']].apply(enc.fit_transform)
df
```

In order to set up the PCA, we needed to further preprocess our data. To do so, we <font color = FFB141>standardized the data outside of the encoded team name</font> using the scikit-learn StandardScaler() method, which calculates a z-score to normalize each value. 


```python
# standardize the data
scaler = preprocessing.StandardScaler()
df.loc[:, ~df.columns.isin(['TEAM_NAME', 'TEAM_NAME.1', 'Home-Team-Win'])] = scaler.fit_transform(df.loc[:, ~df.columns.isin(['TEAM_NAME', 'TEAM_NAME.1', 'Home-Team-Win'])])
pd.set_option('display.max_columns', None)
df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Post-Season</th>
      <th>TEAM_NAME</th>
      <th>GP</th>
      <th>W</th>
      <th>L</th>
      <th>W_PCT</th>
      <th>MIN</th>
      <th>FGM</th>
      <th>FGA</th>
      <th>FG_PCT</th>
      <th>FG3M</th>
      <th>FG3A</th>
      <th>FG3_PCT</th>
      <th>FTM</th>
      <th>FTA</th>
      <th>FT_PCT</th>
      <th>OREB</th>
      <th>DREB</th>
      <th>REB</th>
      <th>AST</th>
      <th>TOV</th>
      <th>STL</th>
      <th>BLK</th>
      <th>BLKA</th>
      <th>PF</th>
      <th>PFD</th>
      <th>PTS</th>
      <th>PLUS_MINUS</th>
      <th>GP_RANK</th>
      <th>W_RANK</th>
      <th>L_RANK</th>
      <th>W_PCT_RANK</th>
      <th>MIN_RANK</th>
      <th>FGM_RANK</th>
      <th>FGA_RANK</th>
      <th>FG_PCT_RANK</th>
      <th>FG3M_RANK</th>
      <th>FG3A_RANK</th>
      <th>FG3_PCT_RANK</th>
      <th>FTM_RANK</th>
      <th>FTA_RANK</th>
      <th>FT_PCT_RANK</th>
      <th>OREB_RANK</th>
      <th>DREB_RANK</th>
      <th>REB_RANK</th>
      <th>AST_RANK</th>
      <th>TOV_RANK</th>
      <th>STL_RANK</th>
      <th>BLK_RANK</th>
      <th>BLKA_RANK</th>
      <th>PF_RANK</th>
      <th>PFD_RANK</th>
      <th>PTS_RANK</th>
      <th>PLUS_MINUS_RANK</th>
      <th>TEAM_NAME.1</th>
      <th>GP.1</th>
      <th>W.1</th>
      <th>L.1</th>
      <th>W_PCT.1</th>
      <th>MIN.1</th>
      <th>FGM.1</th>
      <th>FGA.1</th>
      <th>FG_PCT.1</th>
      <th>FG3M.1</th>
      <th>FG3A.1</th>
      <th>FG3_PCT.1</th>
      <th>FTM.1</th>
      <th>FTA.1</th>
      <th>FT_PCT.1</th>
      <th>OREB.1</th>
      <th>DREB.1</th>
      <th>REB.1</th>
      <th>AST.1</th>
      <th>TOV.1</th>
      <th>STL.1</th>
      <th>BLK.1</th>
      <th>BLKA.1</th>
      <th>PF.1</th>
      <th>PFD.1</th>
      <th>PTS.1</th>
      <th>PLUS_MINUS.1</th>
      <th>GP_RANK.1</th>
      <th>W_RANK.1</th>
      <th>L_RANK.1</th>
      <th>W_PCT_RANK.1</th>
      <th>MIN_RANK.1</th>
      <th>FGM_RANK.1</th>
      <th>FGA_RANK.1</th>
      <th>FG_PCT_RANK.1</th>
      <th>FG3M_RANK.1</th>
      <th>FG3A_RANK.1</th>
      <th>FG3_PCT_RANK.1</th>
      <th>FTM_RANK.1</th>
      <th>FTA_RANK.1</th>
      <th>FT_PCT_RANK.1</th>
      <th>OREB_RANK.1</th>
      <th>DREB_RANK.1</th>
      <th>REB_RANK.1</th>
      <th>AST_RANK.1</th>
      <th>TOV_RANK.1</th>
      <th>STL_RANK.1</th>
      <th>BLK_RANK.1</th>
      <th>BLKA_RANK.1</th>
      <th>PF_RANK.1</th>
      <th>PFD_RANK.1</th>
      <th>PTS_RANK.1</th>
      <th>PLUS_MINUS_RANK.1</th>
      <th>Score</th>
      <th>Home-Team-Win</th>
      <th>OU</th>
      <th>OU-Cover</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-0.237112</td>
      <td>26</td>
      <td>-1.684315</td>
      <td>-1.368008</td>
      <td>-1.466677</td>
      <td>-0.046255</td>
      <td>-1.011219</td>
      <td>-0.707016</td>
      <td>-0.407518</td>
      <td>-0.621131</td>
      <td>-1.672326</td>
      <td>-1.394144</td>
      <td>-2.349628</td>
      <td>0.709227</td>
      <td>1.694739</td>
      <td>-2.152713</td>
      <td>3.942970</td>
      <td>0.567243</td>
      <td>2.841209</td>
      <td>-0.920818</td>
      <td>3.053921</td>
      <td>-1.523315</td>
      <td>3.192008</td>
      <td>0.619164</td>
      <td>-0.658358</td>
      <td>0.447442</td>
      <td>-0.929743</td>
      <td>0.136055</td>
      <td>-1.141731</td>
      <td>-0.719371</td>
      <td>-0.212995</td>
      <td>-0.294941</td>
      <td>-1.039994</td>
      <td>0.307790</td>
      <td>-0.189680</td>
      <td>0.324150</td>
      <td>1.238278</td>
      <td>1.232355</td>
      <td>1.240836</td>
      <td>-0.157160</td>
      <td>-0.278001</td>
      <td>0.652995</td>
      <td>-1.447565</td>
      <td>-0.370527</td>
      <td>-1.191321</td>
      <td>0.303751</td>
      <td>1.234957</td>
      <td>0.425502</td>
      <td>-1.298927</td>
      <td>-0.363763</td>
      <td>-0.972015</td>
      <td>0.535330</td>
      <td>0.549366</td>
      <td>0.002502</td>
      <td>20</td>
      <td>-1.687317</td>
      <td>-1.381122</td>
      <td>-1.466342</td>
      <td>-0.062977</td>
      <td>6.536296</td>
      <td>-3.771622</td>
      <td>-2.234797</td>
      <td>-3.414652</td>
      <td>-0.434619</td>
      <td>-0.332550</td>
      <td>-0.472658</td>
      <td>3.044221</td>
      <td>2.011384</td>
      <td>2.359817</td>
      <td>0.225730</td>
      <td>-1.049705</td>
      <td>-0.889509</td>
      <td>-0.325115</td>
      <td>2.375883</td>
      <td>-1.074764</td>
      <td>1.634716</td>
      <td>-1.040274</td>
      <td>3.201855</td>
      <td>2.145343</td>
      <td>-1.793124</td>
      <td>-2.635598</td>
      <td>-1.134387</td>
      <td>-0.705627</td>
      <td>-0.205234</td>
      <td>-0.283420</td>
      <td>-1.527669</td>
      <td>1.700067</td>
      <td>1.433319</td>
      <td>1.598699</td>
      <td>-0.718621</td>
      <td>-0.962606</td>
      <td>0.211085</td>
      <td>-1.315939</td>
      <td>-0.393304</td>
      <td>-1.428171</td>
      <td>-0.290859</td>
      <td>0.563352</td>
      <td>0.549649</td>
      <td>-0.603831</td>
      <td>0.887215</td>
      <td>0.191459</td>
      <td>-0.949909</td>
      <td>-0.929686</td>
      <td>1.001551</td>
      <td>-0.736910</td>
      <td>1.481850</td>
      <td>1.398772</td>
      <td>-1.154642</td>
      <td>0</td>
      <td>-0.937742</td>
      <td>-0.980049</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.237112</td>
      <td>34</td>
      <td>-1.684315</td>
      <td>-1.432666</td>
      <td>-1.391912</td>
      <td>-2.758043</td>
      <td>6.490512</td>
      <td>-2.532929</td>
      <td>1.278868</td>
      <td>-4.872798</td>
      <td>-2.026456</td>
      <td>-0.863886</td>
      <td>-6.426255</td>
      <td>4.981527</td>
      <td>5.535196</td>
      <td>-0.664518</td>
      <td>5.290768</td>
      <td>-0.233890</td>
      <td>2.841209</td>
      <td>-3.745270</td>
      <td>3.053921</td>
      <td>-1.523315</td>
      <td>0.560961</td>
      <td>1.171527</td>
      <td>3.166770</td>
      <td>4.446446</td>
      <td>-0.929743</td>
      <td>-2.702999</td>
      <td>-1.141731</td>
      <td>0.666844</td>
      <td>1.052828</td>
      <td>0.621091</td>
      <td>-1.528895</td>
      <td>1.461772</td>
      <td>-1.339362</td>
      <td>1.705861</td>
      <td>1.583708</td>
      <td>0.077321</td>
      <td>1.701155</td>
      <td>-1.543243</td>
      <td>-1.544951</td>
      <td>-0.154675</td>
      <td>-1.677739</td>
      <td>0.089100</td>
      <td>-1.191321</td>
      <td>1.679915</td>
      <td>1.234957</td>
      <td>0.425502</td>
      <td>-0.034076</td>
      <td>-0.248835</td>
      <td>0.991031</td>
      <td>-1.546490</td>
      <td>0.549366</td>
      <td>1.493426</td>
      <td>25</td>
      <td>-1.687317</td>
      <td>-1.381122</td>
      <td>-1.466342</td>
      <td>-0.062977</td>
      <td>-1.021857</td>
      <td>-2.754641</td>
      <td>-2.234797</td>
      <td>-1.744681</td>
      <td>0.454109</td>
      <td>-0.598678</td>
      <td>4.948056</td>
      <td>2.618718</td>
      <td>3.503699</td>
      <td>-1.558520</td>
      <td>-0.780603</td>
      <td>-1.653157</td>
      <td>-2.064819</td>
      <td>-2.758332</td>
      <td>-2.521764</td>
      <td>-3.846852</td>
      <td>0.564846</td>
      <td>1.205498</td>
      <td>0.174232</td>
      <td>2.713104</td>
      <td>-0.868588</td>
      <td>-0.522177</td>
      <td>-1.134387</td>
      <td>-0.705627</td>
      <td>-0.205234</td>
      <td>-0.283420</td>
      <td>-1.037618</td>
      <td>1.584905</td>
      <td>1.433319</td>
      <td>0.679182</td>
      <td>-1.409793</td>
      <td>-0.271247</td>
      <td>-1.406621</td>
      <td>-0.969085</td>
      <td>-1.200365</td>
      <td>0.418330</td>
      <td>0.629332</td>
      <td>0.909407</td>
      <td>1.126565</td>
      <td>1.465104</td>
      <td>-1.658707</td>
      <td>1.572071</td>
      <td>-0.026738</td>
      <td>-0.237605</td>
      <td>-0.616111</td>
      <td>-1.084122</td>
      <td>0.442852</td>
      <td>0.132115</td>
      <td>-1.376722</td>
      <td>0</td>
      <td>-0.359796</td>
      <td>-0.980049</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-0.237112</td>
      <td>16</td>
      <td>-1.725624</td>
      <td>-1.432666</td>
      <td>-1.466677</td>
      <td>-2.758043</td>
      <td>-1.011219</td>
      <td>-0.098379</td>
      <td>-1.009799</td>
      <td>1.049166</td>
      <td>0.452448</td>
      <td>-0.201062</td>
      <td>2.754719</td>
      <td>-1.213308</td>
      <td>-1.143860</td>
      <td>-0.333808</td>
      <td>-3.132971</td>
      <td>1.368377</td>
      <td>-0.481454</td>
      <td>-0.517325</td>
      <td>-0.424970</td>
      <td>-2.438365</td>
      <td>1.087170</td>
      <td>0.066801</td>
      <td>-0.931581</td>
      <td>-2.123346</td>
      <td>-0.290332</td>
      <td>-0.596604</td>
      <td>1.337486</td>
      <td>0.666844</td>
      <td>-0.212995</td>
      <td>0.621091</td>
      <td>-1.039994</td>
      <td>-0.499998</td>
      <td>0.500130</td>
      <td>-0.712132</td>
      <td>-1.410018</td>
      <td>-1.193216</td>
      <td>-0.600438</td>
      <td>0.651389</td>
      <td>0.873771</td>
      <td>-0.385437</td>
      <td>1.544701</td>
      <td>-0.945061</td>
      <td>0.307022</td>
      <td>-0.499011</td>
      <td>-0.850527</td>
      <td>1.115723</td>
      <td>-0.609008</td>
      <td>-0.708545</td>
      <td>-1.202962</td>
      <td>1.460584</td>
      <td>-0.027929</td>
      <td>0.231874</td>
      <td>12</td>
      <td>-1.687317</td>
      <td>-1.316067</td>
      <td>-1.541316</td>
      <td>2.672370</td>
      <td>6.536296</td>
      <td>-1.127471</td>
      <td>0.795803</td>
      <td>-2.453154</td>
      <td>0.454109</td>
      <td>-0.332550</td>
      <td>3.413892</td>
      <td>1.767712</td>
      <td>1.679758</td>
      <td>0.275596</td>
      <td>1.902951</td>
      <td>2.772158</td>
      <td>3.811730</td>
      <td>-0.527883</td>
      <td>0.626723</td>
      <td>2.621355</td>
      <td>4.844328</td>
      <td>2.889828</td>
      <td>2.651378</td>
      <td>1.009821</td>
      <td>-0.015169</td>
      <td>0.948030</td>
      <td>-1.134387</td>
      <td>-1.400286</td>
      <td>-1.475335</td>
      <td>-1.545970</td>
      <td>-1.527669</td>
      <td>0.663610</td>
      <td>-1.107667</td>
      <td>1.138941</td>
      <td>-1.409793</td>
      <td>-0.962606</td>
      <td>-0.828869</td>
      <td>-0.622231</td>
      <td>-0.278009</td>
      <td>-0.851140</td>
      <td>-0.520907</td>
      <td>-1.628331</td>
      <td>-1.642633</td>
      <td>-0.488890</td>
      <td>-0.038575</td>
      <td>-1.304205</td>
      <td>-1.642288</td>
      <td>0.800517</td>
      <td>0.770456</td>
      <td>-0.042486</td>
      <td>-0.596146</td>
      <td>-0.443638</td>
      <td>1.110574</td>
      <td>0</td>
      <td>0.218150</td>
      <td>0.925933</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-0.237112</td>
      <td>18</td>
      <td>-1.684315</td>
      <td>-1.432666</td>
      <td>-1.391912</td>
      <td>-2.758043</td>
      <td>-1.011219</td>
      <td>-0.301258</td>
      <td>0.676587</td>
      <td>-1.177897</td>
      <td>-1.495262</td>
      <td>-1.261579</td>
      <td>-1.870025</td>
      <td>-2.708613</td>
      <td>-1.811765</td>
      <td>-3.392875</td>
      <td>3.606020</td>
      <td>-1.035024</td>
      <td>1.082152</td>
      <td>-1.727804</td>
      <td>1.662365</td>
      <td>-1.980840</td>
      <td>1.087170</td>
      <td>3.380980</td>
      <td>5.079334</td>
      <td>1.018729</td>
      <td>-1.711245</td>
      <td>-2.061923</td>
      <td>-1.141731</td>
      <td>0.666844</td>
      <td>1.052828</td>
      <td>0.621091</td>
      <td>-1.039994</td>
      <td>-0.153803</td>
      <td>-0.994457</td>
      <td>0.554435</td>
      <td>1.123135</td>
      <td>0.654838</td>
      <td>1.010677</td>
      <td>1.459937</td>
      <td>1.334480</td>
      <td>1.345284</td>
      <td>-1.332478</td>
      <td>0.548726</td>
      <td>-0.615035</td>
      <td>1.221193</td>
      <td>0.539796</td>
      <td>0.885649</td>
      <td>-0.609008</td>
      <td>1.015365</td>
      <td>1.683871</td>
      <td>-0.042953</td>
      <td>1.242120</td>
      <td>1.034680</td>
      <td>5</td>
      <td>-1.687317</td>
      <td>-1.446177</td>
      <td>-1.391368</td>
      <td>-2.798324</td>
      <td>6.536296</td>
      <td>-1.127471</td>
      <td>1.644371</td>
      <td>-3.262836</td>
      <td>-0.434619</td>
      <td>-0.132954</td>
      <td>-1.222694</td>
      <td>-0.998059</td>
      <td>-0.309996</td>
      <td>-2.030943</td>
      <td>3.244728</td>
      <td>-3.061211</td>
      <td>-1.085394</td>
      <td>-0.730651</td>
      <td>0.976555</td>
      <td>1.235310</td>
      <td>0.029911</td>
      <td>2.328385</td>
      <td>3.201855</td>
      <td>3.280865</td>
      <td>-1.295297</td>
      <td>-1.900495</td>
      <td>-1.134387</td>
      <td>0.683692</td>
      <td>1.064867</td>
      <td>0.634799</td>
      <td>-1.527669</td>
      <td>0.663610</td>
      <td>-1.569665</td>
      <td>1.483759</td>
      <td>-0.718621</td>
      <td>-1.308285</td>
      <td>0.673287</td>
      <td>0.533948</td>
      <td>0.413758</td>
      <td>0.533737</td>
      <td>-1.211050</td>
      <td>1.601518</td>
      <td>0.780415</td>
      <td>0.085814</td>
      <td>0.077149</td>
      <td>-0.728950</td>
      <td>0.434848</td>
      <td>0.569823</td>
      <td>1.001551</td>
      <td>-1.315597</td>
      <td>0.904629</td>
      <td>0.938170</td>
      <td>-2.531538</td>
      <td>1</td>
      <td>-0.614092</td>
      <td>-0.980049</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-0.237112</td>
      <td>11</td>
      <td>-1.684315</td>
      <td>-1.303349</td>
      <td>-1.541443</td>
      <td>2.665533</td>
      <td>-1.011219</td>
      <td>-0.707016</td>
      <td>-1.732535</td>
      <td>0.998551</td>
      <td>-1.141133</td>
      <td>-0.731321</td>
      <td>-2.246856</td>
      <td>1.777302</td>
      <td>2.362645</td>
      <td>-1.133024</td>
      <td>-0.100425</td>
      <td>0.767527</td>
      <td>0.691250</td>
      <td>-0.517325</td>
      <td>1.314475</td>
      <td>1.679359</td>
      <td>1.613380</td>
      <td>-1.590289</td>
      <td>3.439994</td>
      <td>2.161301</td>
      <td>-0.361378</td>
      <td>1.143461</td>
      <td>-1.141731</td>
      <td>-1.412478</td>
      <td>-1.478819</td>
      <td>-1.554484</td>
      <td>-1.039994</td>
      <td>0.307790</td>
      <td>1.074971</td>
      <td>-0.596990</td>
      <td>0.892848</td>
      <td>-0.153686</td>
      <td>1.125757</td>
      <td>-0.619188</td>
      <td>-0.623533</td>
      <td>0.191470</td>
      <td>0.163655</td>
      <td>-0.715248</td>
      <td>-0.269264</td>
      <td>-0.499011</td>
      <td>0.308075</td>
      <td>-0.954941</td>
      <td>-0.953968</td>
      <td>-1.168254</td>
      <td>1.452924</td>
      <td>-0.736893</td>
      <td>0.318448</td>
      <td>-0.685617</td>
      <td>28</td>
      <td>-1.687317</td>
      <td>-1.446177</td>
      <td>-1.391368</td>
      <td>-2.798324</td>
      <td>-1.021857</td>
      <td>0.499699</td>
      <td>-0.901333</td>
      <td>1.848285</td>
      <td>-0.967855</td>
      <td>-1.530126</td>
      <td>3.652540</td>
      <td>-3.338327</td>
      <td>-3.294627</td>
      <td>-0.585883</td>
      <td>-1.451491</td>
      <td>-2.457759</td>
      <td>-3.240128</td>
      <td>-1.338955</td>
      <td>0.276891</td>
      <td>-4.770882</td>
      <td>-2.644765</td>
      <td>-1.601718</td>
      <td>-1.201961</td>
      <td>-0.693462</td>
      <td>-1.153060</td>
      <td>-2.727486</td>
      <td>-1.134387</td>
      <td>0.683692</td>
      <td>1.064867</td>
      <td>0.634799</td>
      <td>-1.037618</td>
      <td>-0.948657</td>
      <td>0.278325</td>
      <td>-1.274790</td>
      <td>0.548527</td>
      <td>1.341924</td>
      <td>-1.059970</td>
      <td>1.574510</td>
      <td>1.681999</td>
      <td>-0.274108</td>
      <td>1.089428</td>
      <td>1.486166</td>
      <td>1.588098</td>
      <td>0.890399</td>
      <td>-0.617193</td>
      <td>1.687122</td>
      <td>1.588812</td>
      <td>-1.160380</td>
      <td>-1.540490</td>
      <td>0.767676</td>
      <td>0.789185</td>
      <td>1.513923</td>
      <td>-1.687634</td>
      <td>1</td>
      <td>-0.151735</td>
      <td>-0.980049</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>18838</th>
      <td>4.217421</td>
      <td>10</td>
      <td>1.620342</td>
      <td>1.994246</td>
      <td>0.626761</td>
      <td>0.745587</td>
      <td>-0.711150</td>
      <td>0.916017</td>
      <td>0.532040</td>
      <td>0.745476</td>
      <td>1.975203</td>
      <td>1.972998</td>
      <td>0.322446</td>
      <td>-0.956970</td>
      <td>-1.043674</td>
      <td>0.189816</td>
      <td>-0.572154</td>
      <td>1.248207</td>
      <td>0.886701</td>
      <td>1.943984</td>
      <td>0.201230</td>
      <td>1.038824</td>
      <td>-0.491458</td>
      <td>-1.148398</td>
      <td>0.161312</td>
      <td>-1.552059</td>
      <td>1.130581</td>
      <td>0.960297</td>
      <td>-1.254422</td>
      <td>-1.296960</td>
      <td>-1.248669</td>
      <td>-1.325476</td>
      <td>1.282285</td>
      <td>0.192392</td>
      <td>0.615098</td>
      <td>-0.596990</td>
      <td>-1.410018</td>
      <td>-1.424223</td>
      <td>-0.830597</td>
      <td>1.228923</td>
      <td>1.219303</td>
      <td>0.191470</td>
      <td>0.508917</td>
      <td>-1.519595</td>
      <td>-0.960806</td>
      <td>-1.187093</td>
      <td>1.582538</td>
      <td>-1.300052</td>
      <td>0.310884</td>
      <td>-1.513036</td>
      <td>1.337451</td>
      <td>1.576241</td>
      <td>-0.027929</td>
      <td>-1.144363</td>
      <td>1</td>
      <td>1.622118</td>
      <td>1.871645</td>
      <td>0.782869</td>
      <td>0.604447</td>
      <td>0.489774</td>
      <td>0.987849</td>
      <td>0.771558</td>
      <td>0.583156</td>
      <td>1.591681</td>
      <td>1.676717</td>
      <td>0.038730</td>
      <td>-0.359805</td>
      <td>-0.840597</td>
      <td>1.498339</td>
      <td>-0.109714</td>
      <td>1.162952</td>
      <td>1.108518</td>
      <td>1.013154</td>
      <td>-0.702638</td>
      <td>-0.427943</td>
      <td>0.885807</td>
      <td>-0.366543</td>
      <td>-1.201961</td>
      <td>-0.750238</td>
      <td>1.236510</td>
      <td>1.278826</td>
      <td>-1.247306</td>
      <td>-0.937180</td>
      <td>-0.898016</td>
      <td>-0.972084</td>
      <td>-1.405156</td>
      <td>-0.257686</td>
      <td>0.278325</td>
      <td>-0.010455</td>
      <td>-0.833816</td>
      <td>-0.732153</td>
      <td>-0.135566</td>
      <td>-0.159759</td>
      <td>0.990231</td>
      <td>-1.543578</td>
      <td>-0.520907</td>
      <td>-1.282276</td>
      <td>-1.181100</td>
      <td>-0.144068</td>
      <td>-0.270022</td>
      <td>0.421561</td>
      <td>-1.526892</td>
      <td>-0.468299</td>
      <td>-1.193848</td>
      <td>0.536201</td>
      <td>-0.365258</td>
      <td>-1.479995</td>
      <td>-0.532818</td>
      <td>1</td>
      <td>0.264386</td>
      <td>-0.980049</td>
    </tr>
    <tr>
      <th>18839</th>
      <td>4.217421</td>
      <td>1</td>
      <td>1.620342</td>
      <td>1.864929</td>
      <td>0.776292</td>
      <td>0.615421</td>
      <td>0.489127</td>
      <td>0.997169</td>
      <td>0.772952</td>
      <td>0.593631</td>
      <td>1.585661</td>
      <td>1.668099</td>
      <td>0.048387</td>
      <td>-0.358848</td>
      <td>-0.843302</td>
      <td>1.485097</td>
      <td>-0.100425</td>
      <td>1.168094</td>
      <td>1.121242</td>
      <td>1.015949</td>
      <td>-0.703281</td>
      <td>-0.425255</td>
      <td>0.876686</td>
      <td>-0.375090</td>
      <td>-1.204805</td>
      <td>-0.752259</td>
      <td>1.244254</td>
      <td>1.289993</td>
      <td>-1.254422</td>
      <td>-0.950407</td>
      <td>-0.903444</td>
      <td>-0.981964</td>
      <td>-1.406670</td>
      <td>-0.269201</td>
      <td>0.270193</td>
      <td>-0.021277</td>
      <td>-0.834302</td>
      <td>-0.731203</td>
      <td>-0.140120</td>
      <td>-0.157160</td>
      <td>0.988948</td>
      <td>-1.539251</td>
      <td>-0.526867</td>
      <td>-1.289781</td>
      <td>-1.191321</td>
      <td>-0.154970</td>
      <td>-0.271226</td>
      <td>0.425502</td>
      <td>-1.528900</td>
      <td>-0.478690</td>
      <td>-1.202962</td>
      <td>0.535330</td>
      <td>-0.374306</td>
      <td>-1.488423</td>
      <td>10</td>
      <td>1.622118</td>
      <td>2.001756</td>
      <td>0.632922</td>
      <td>0.735744</td>
      <td>-0.719531</td>
      <td>0.906491</td>
      <td>0.529110</td>
      <td>0.734971</td>
      <td>1.982721</td>
      <td>1.982764</td>
      <td>0.311470</td>
      <td>-0.955509</td>
      <td>-1.039572</td>
      <td>0.192227</td>
      <td>-0.579336</td>
      <td>1.243413</td>
      <td>0.873456</td>
      <td>1.945886</td>
      <td>0.206925</td>
      <td>1.050504</td>
      <td>-0.505025</td>
      <td>-1.152563</td>
      <td>0.174232</td>
      <td>-1.545104</td>
      <td>1.122721</td>
      <td>0.948030</td>
      <td>-1.247306</td>
      <td>-1.284510</td>
      <td>-1.244408</td>
      <td>-1.316416</td>
      <td>1.290127</td>
      <td>0.202962</td>
      <td>0.624823</td>
      <td>-0.585153</td>
      <td>-1.409793</td>
      <td>-1.423512</td>
      <td>-0.828869</td>
      <td>1.227656</td>
      <td>1.220820</td>
      <td>0.187518</td>
      <td>0.514308</td>
      <td>-1.512979</td>
      <td>-0.950333</td>
      <td>-1.178535</td>
      <td>1.581558</td>
      <td>-1.304205</td>
      <td>0.319452</td>
      <td>-1.506420</td>
      <td>1.348193</td>
      <td>1.577838</td>
      <td>-0.018925</td>
      <td>-1.134543</td>
      <td>0.399918</td>
      <td>1</td>
      <td>0.264386</td>
      <td>0.925933</td>
    </tr>
    <tr>
      <th>18840</th>
      <td>4.217421</td>
      <td>1</td>
      <td>1.620342</td>
      <td>1.864929</td>
      <td>0.776292</td>
      <td>0.615421</td>
      <td>0.489127</td>
      <td>0.997169</td>
      <td>0.772952</td>
      <td>0.593631</td>
      <td>1.585661</td>
      <td>1.668099</td>
      <td>0.048387</td>
      <td>-0.358848</td>
      <td>-0.843302</td>
      <td>1.485097</td>
      <td>-0.100425</td>
      <td>1.168094</td>
      <td>1.121242</td>
      <td>1.015949</td>
      <td>-0.703281</td>
      <td>-0.425255</td>
      <td>0.876686</td>
      <td>-0.375090</td>
      <td>-1.204805</td>
      <td>-0.752259</td>
      <td>1.244254</td>
      <td>1.289993</td>
      <td>-1.254422</td>
      <td>-0.950407</td>
      <td>-0.903444</td>
      <td>-0.981964</td>
      <td>-1.406670</td>
      <td>-0.269201</td>
      <td>0.270193</td>
      <td>-0.021277</td>
      <td>-0.834302</td>
      <td>-0.731203</td>
      <td>-0.140120</td>
      <td>-0.157160</td>
      <td>0.988948</td>
      <td>-1.539251</td>
      <td>-0.526867</td>
      <td>-1.289781</td>
      <td>-1.191321</td>
      <td>-0.154970</td>
      <td>-0.271226</td>
      <td>0.425502</td>
      <td>-1.528900</td>
      <td>-0.478690</td>
      <td>-1.202962</td>
      <td>0.535330</td>
      <td>-0.374306</td>
      <td>-1.488423</td>
      <td>10</td>
      <td>1.622118</td>
      <td>2.001756</td>
      <td>0.632922</td>
      <td>0.735744</td>
      <td>-0.719531</td>
      <td>0.906491</td>
      <td>0.529110</td>
      <td>0.734971</td>
      <td>1.982721</td>
      <td>1.982764</td>
      <td>0.311470</td>
      <td>-0.955509</td>
      <td>-1.039572</td>
      <td>0.192227</td>
      <td>-0.579336</td>
      <td>1.243413</td>
      <td>0.873456</td>
      <td>1.945886</td>
      <td>0.206925</td>
      <td>1.050504</td>
      <td>-0.505025</td>
      <td>-1.152563</td>
      <td>0.174232</td>
      <td>-1.545104</td>
      <td>1.122721</td>
      <td>0.948030</td>
      <td>-1.247306</td>
      <td>-1.284510</td>
      <td>-1.244408</td>
      <td>-1.316416</td>
      <td>1.290127</td>
      <td>0.202962</td>
      <td>0.624823</td>
      <td>-0.585153</td>
      <td>-1.409793</td>
      <td>-1.423512</td>
      <td>-0.828869</td>
      <td>1.227656</td>
      <td>1.220820</td>
      <td>0.187518</td>
      <td>0.514308</td>
      <td>-1.512979</td>
      <td>-0.950333</td>
      <td>-1.178535</td>
      <td>1.581558</td>
      <td>-1.304205</td>
      <td>0.319452</td>
      <td>-1.506420</td>
      <td>1.348193</td>
      <td>1.577838</td>
      <td>-0.018925</td>
      <td>-1.134543</td>
      <td>-0.133074</td>
      <td>0</td>
      <td>0.264386</td>
      <td>-0.980049</td>
    </tr>
    <tr>
      <th>18841</th>
      <td>4.217421</td>
      <td>10</td>
      <td>1.620342</td>
      <td>1.994246</td>
      <td>0.626761</td>
      <td>0.745587</td>
      <td>-0.711150</td>
      <td>0.916017</td>
      <td>0.532040</td>
      <td>0.745476</td>
      <td>1.975203</td>
      <td>1.972998</td>
      <td>0.322446</td>
      <td>-0.956970</td>
      <td>-1.043674</td>
      <td>0.189816</td>
      <td>-0.572154</td>
      <td>1.248207</td>
      <td>0.886701</td>
      <td>1.943984</td>
      <td>0.201230</td>
      <td>1.038824</td>
      <td>-0.491458</td>
      <td>-1.148398</td>
      <td>0.161312</td>
      <td>-1.552059</td>
      <td>1.130581</td>
      <td>0.960297</td>
      <td>-1.254422</td>
      <td>-1.296960</td>
      <td>-1.248669</td>
      <td>-1.325476</td>
      <td>1.282285</td>
      <td>0.192392</td>
      <td>0.615098</td>
      <td>-0.596990</td>
      <td>-1.410018</td>
      <td>-1.424223</td>
      <td>-0.830597</td>
      <td>1.228923</td>
      <td>1.219303</td>
      <td>0.191470</td>
      <td>0.508917</td>
      <td>-1.519595</td>
      <td>-0.960806</td>
      <td>-1.187093</td>
      <td>1.582538</td>
      <td>-1.300052</td>
      <td>0.310884</td>
      <td>-1.513036</td>
      <td>1.337451</td>
      <td>1.576241</td>
      <td>-0.027929</td>
      <td>-1.144363</td>
      <td>1</td>
      <td>1.622118</td>
      <td>1.871645</td>
      <td>0.782869</td>
      <td>0.604447</td>
      <td>0.489774</td>
      <td>0.987849</td>
      <td>0.771558</td>
      <td>0.583156</td>
      <td>1.591681</td>
      <td>1.676717</td>
      <td>0.038730</td>
      <td>-0.359805</td>
      <td>-0.840597</td>
      <td>1.498339</td>
      <td>-0.109714</td>
      <td>1.162952</td>
      <td>1.108518</td>
      <td>1.013154</td>
      <td>-0.702638</td>
      <td>-0.427943</td>
      <td>0.885807</td>
      <td>-0.366543</td>
      <td>-1.201961</td>
      <td>-0.750238</td>
      <td>1.236510</td>
      <td>1.278826</td>
      <td>-1.247306</td>
      <td>-0.937180</td>
      <td>-0.898016</td>
      <td>-0.972084</td>
      <td>-1.405156</td>
      <td>-0.257686</td>
      <td>0.278325</td>
      <td>-0.010455</td>
      <td>-0.833816</td>
      <td>-0.732153</td>
      <td>-0.135566</td>
      <td>-0.159759</td>
      <td>0.990231</td>
      <td>-1.543578</td>
      <td>-0.520907</td>
      <td>-1.282276</td>
      <td>-1.181100</td>
      <td>-0.144068</td>
      <td>-0.270022</td>
      <td>0.421561</td>
      <td>-1.526892</td>
      <td>-0.468299</td>
      <td>-1.193848</td>
      <td>0.536201</td>
      <td>-0.365258</td>
      <td>-1.479995</td>
      <td>-0.399570</td>
      <td>1</td>
      <td>0.264386</td>
      <td>-0.980049</td>
    </tr>
    <tr>
      <th>18842</th>
      <td>4.217421</td>
      <td>1</td>
      <td>1.620342</td>
      <td>1.864929</td>
      <td>0.776292</td>
      <td>0.615421</td>
      <td>0.489127</td>
      <td>0.997169</td>
      <td>0.772952</td>
      <td>0.593631</td>
      <td>1.585661</td>
      <td>1.668099</td>
      <td>0.048387</td>
      <td>-0.358848</td>
      <td>-0.843302</td>
      <td>1.485097</td>
      <td>-0.100425</td>
      <td>1.168094</td>
      <td>1.121242</td>
      <td>1.015949</td>
      <td>-0.703281</td>
      <td>-0.425255</td>
      <td>0.876686</td>
      <td>-0.375090</td>
      <td>-1.204805</td>
      <td>-0.752259</td>
      <td>1.244254</td>
      <td>1.289993</td>
      <td>-1.254422</td>
      <td>-0.950407</td>
      <td>-0.903444</td>
      <td>-0.981964</td>
      <td>-1.406670</td>
      <td>-0.269201</td>
      <td>0.270193</td>
      <td>-0.021277</td>
      <td>-0.834302</td>
      <td>-0.731203</td>
      <td>-0.140120</td>
      <td>-0.157160</td>
      <td>0.988948</td>
      <td>-1.539251</td>
      <td>-0.526867</td>
      <td>-1.289781</td>
      <td>-1.191321</td>
      <td>-0.154970</td>
      <td>-0.271226</td>
      <td>0.425502</td>
      <td>-1.528900</td>
      <td>-0.478690</td>
      <td>-1.202962</td>
      <td>0.535330</td>
      <td>-0.374306</td>
      <td>-1.488423</td>
      <td>10</td>
      <td>1.622118</td>
      <td>2.001756</td>
      <td>0.632922</td>
      <td>0.735744</td>
      <td>-0.719531</td>
      <td>0.906491</td>
      <td>0.529110</td>
      <td>0.734971</td>
      <td>1.982721</td>
      <td>1.982764</td>
      <td>0.311470</td>
      <td>-0.955509</td>
      <td>-1.039572</td>
      <td>0.192227</td>
      <td>-0.579336</td>
      <td>1.243413</td>
      <td>0.873456</td>
      <td>1.945886</td>
      <td>0.206925</td>
      <td>1.050504</td>
      <td>-0.505025</td>
      <td>-1.152563</td>
      <td>0.174232</td>
      <td>-1.545104</td>
      <td>1.122721</td>
      <td>0.948030</td>
      <td>-1.247306</td>
      <td>-1.284510</td>
      <td>-1.244408</td>
      <td>-1.316416</td>
      <td>1.290127</td>
      <td>0.202962</td>
      <td>0.624823</td>
      <td>-0.585153</td>
      <td>-1.409793</td>
      <td>-1.423512</td>
      <td>-0.828869</td>
      <td>1.227656</td>
      <td>1.220820</td>
      <td>0.187518</td>
      <td>0.514308</td>
      <td>-1.512979</td>
      <td>-0.950333</td>
      <td>-1.178535</td>
      <td>1.581558</td>
      <td>-1.304205</td>
      <td>0.319452</td>
      <td>-1.506420</td>
      <td>1.348193</td>
      <td>1.577838</td>
      <td>-0.018925</td>
      <td>-1.134543</td>
      <td>-0.621650</td>
      <td>0</td>
      <td>0.264386</td>
      <td>-0.980049</td>
    </tr>
  </tbody>
</table>
<p>18843 rows × 111 columns</p>
</div>



### Pre-Processing

To conduct PCA, we drop five columns (Team_Name, Team_Name.1, Score, Home-Team-Win, and OU-Cover). We set our variance threshold to 95%, and using scikit-learn’s PCA object, we fit_transform our dataset to 39 principal components. Next, a boolean variable do_PCA is set to decide if we want to include PCA components or our raw components in model training. This is useful for comparing model performance between with and without PCA.


```python
# PCA but exclude the categorical team 
df_PCA = df.drop(columns=['TEAM_NAME', 'TEAM_NAME.1', 'Score', 'OU-Cover', 'Home-Team-Win'])
pca = decomposition.PCA(n_components=0.95)
df_PCA_transformed = pca.fit_transform(df_PCA)
print(f'Below is an array of the new components sorted by decreasing explained variance:')
print(pca.explained_variance_ratio_)
print(f'New # of features is: {len(pca.explained_variance_ratio_)}')
print(df_PCA_transformed.shape[1])
```


```python
do_PCA = False

if do_PCA:
    arr = np.array(df_PCA_transformed)
    labels = np.array(df['Home-Team-Win']).reshape((-1, 1))
    arr = np.hstack((arr, labels))
else:
    arr = np.array(df_PCA)
    labels = np.array(df['Home-Team-Win']).reshape((-1, 1))
    arr = np.hstack((arr, labels))
# labels added to the end
arr[:5, -5:]
```

Before running our feedforward / basic neural network, we first split our data into training and test sections, randomized on the basis of date - using a 90/10 split. We also remove the dependent label: the “Home-Team Win” field into a new array to simplify our code. 


```python
# randomize data based on dates
rand = arr.copy()
np.random.shuffle(rand)
rand
```

### Model Design

In designing our feedforward neural net, we made several design choices. First, we tested between 2-4 layers of tensorflow’s Dense neurons to see how model performance changed. X layers was the optimal number. Based on online research and performance, a rectified linear unit (relu) activation function appeared to be optimal for initial and intermediary layers, while the output neuron is one with a sigmoid activation function for binary classification.


```python
# remove y label (home team winning)
y = rand[:, -1]
x = np.delete(rand, -1, axis=1)

# take a % of data for training
percent = 0.90
length = len(rand)
x_train = x[:int(percent * length), :]
y_train = y[:int(percent * length)]
x_test = x[int(percent * length):, :]
y_test = y[int(percent * length):]
```


```python
x_train.shape
```




    (16958, 106)




```python
# basic neural net to see if it works
model = models.Sequential([
    layers.Dense(units=100, activation='relu', input_shape=(x_train.shape[1], )),
    layers.Dense(units=1, activation='sigmoid')
])
model.summary()
```

    Model: "sequential"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     dense (Dense)               (None, 100)               10700     
                                                                     
     dense_1 (Dense)             (None, 1)                 101       
                                                                     
    =================================================================
    Total params: 10,801
    Trainable params: 10,801
    Non-trainable params: 0
    _________________________________________________________________
    


```python
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
losses_data = model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs=40)

```

    530/530 [==============================] - 2s 3ms/step - loss: 0.4067 - acc: 0.8133 - val_loss: 0.8580 - val_acc: 0.6159
    Epoch 39/40
    530/530 [==============================] - 1s 2ms/step - loss: 0.4033 - acc: 0.8146 - val_loss: 0.8557 - val_acc: 0.6345
    Epoch 40/40
    530/530 [==============================] - 1s 2ms/step - loss: 0.4032 - acc: 0.8128 - val_loss: 0.8767 - val_acc: 0.5989
    

We run the model with the Adam optimizer and the MSE loss function.  Below  are displayed the loss functions. The loss is the loss of our model on training data, while the val_loss is the loss during cross_validation on testing data.


```python
loss = pd.DataFrame(losses_data.history)
loss
loss.loc[:, ['loss', 'val_loss']].plot()
```




    <AxesSubplot:>




    
![png](CodeV1_files/CodeV1_18_1.png)
    


We also see the accuracy score, or the percentage of correctly predicted winners over all games in the dataset. This value of 0.63 suggests the model is relatively adept at correctly classifying winners. 


We can also view the Mean Squared Error (MSE) in this regression context, allowing us to see the average of the squared difference between the predicted and actual outcomes. In this case, the predicted outcome can be the probability of the home team winning the game. The MSE value of 0.37 shows the model is effective at reducing error. 


Below we see the Area Under the Receiver Operating Characteristic Curve (AUC-ROC), commonly used for binary classification problems to measure the performance of the neural network in terms of the trade-off between true positive rate and false positive rate. In this case, the predicted outcome can be the probability of the home team winning the game. Since the AUC value of the ROC curve is relatively close to 1 (.64), the model is a good classifier. 



```python
# Calculating F1 score
pred = model.predict(x_test)
binary_pred = (pred > 0.5).astype(np.float32)

#metrics.confusion_matrix(y_test, model.predict(x_test))

# do accuracy, confusion matrix, F1 score

f1 = metrics.f1_score(y_test, binary_pred)
print("F1 Score: {:.2f}".format(f1))

acc = accuracy_score(y_test, binary_pred)

print("Accuracy: {:.2f}".format(acc))

mse = mean_squared_error(y_test, binary_pred)

print("MSE: {:.2f}".format(mse))

# generate ROC curve
fpr, tpr, thresholds = roc_curve(y_test, pred)

# calculate AUC
auc = roc_auc_score(y_test, pred)

print("AUC: {:.2f}".format(auc))

# plot ROC curve
plt.plot(fpr, tpr)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.show()
```

    59/59 [==============================] - 0s 2ms/step
    [[0.6515496]
     [0.6637979]
     [0.5518583]
     ...
     [0.9055562]
     [0.5784015]
     [0.8574344]]
    F1 Score: 0.71
    Accuracy: 0.65
    MSE: 0.35
    AUC: 0.67
    


    
![png](CodeV1_files/CodeV1_20_1.png)
    



